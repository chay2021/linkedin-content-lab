# Your Kubernetes Probes Might Be Causing Your Outages.


ğŸ‘‰ If you think liveness/readiness probes are â€œjust best practiceâ€â€¦ you might be the reason your production pipeline keeps restarting.

Yes â€” I said it.

Because the fastest way to turn a healthy system into a disaster is:

Kill it repeatedly while itâ€™s recovering.


A very real day-to-day failure
Your pipeline is simple:

Kafka â†’ Consumers (K8s) â†’ Elasticsearch â†’ Dashboards

Elasticsearch slows down for 2 minutes (merges, GC, burst traffic). Your consumer starts taking longer to process.

Then Kubernetes says:

âœ… â€œProbe failed.â€ 

ğŸ’¥ â€œRestart pod.â€ 

â™»ï¸ Consumer group rebalances. 

ğŸ“ˆ Lag spikes harder. 

ğŸ’¥ More probe failures. 

ğŸ” Restart storm.

Kafka isnâ€™t down. Your pods are being executed on a schedule.


What we think probes do
âŒ What we think
â€œProbes detect failure and heal the app.â€

âœ… What they actually do
â€œProbes decide when to kill your app.â€

And if you misconfigure them, Kubernetes becomes your most aggressive attacker.


The intelligence test
If your consumer is slow because the sink is slowâ€¦

ğŸ‘‰ Should the fix be â€œrestart itâ€?

Or should the fix be:

backpressure

timeouts

retries with delay

throttling

letting it drain safely

If your answer is â€œrestartâ€â€¦ youâ€™re treating symptoms with a shotgun.


One line you should remember
Readiness = â€œDonâ€™t send traffic yet.â€ Liveness = â€œKill it.â€

If you confuse the two, you donâ€™t have resilience. You have roulette.


ğŸ‘‡ Hot take: Have you ever seen a â€œprobe configurationâ€ create a bigger outage than the actual issue?

Experienced engineers â€” I know youâ€™ve seen this.
