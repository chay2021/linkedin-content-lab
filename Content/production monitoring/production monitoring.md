# Monitoring Kafka, NiFi & Elasticsearch Like a Production Engineer

Ever been in a pipeline incident where everyone ran to dashboardsâ€¦

â€¦and still nobody could answer the simplest question:

ğŸ‘‰ â€œWhere is the pipeline stuck?â€

Because everything looked fine:

CPU wasnâ€™t maxed

Kafka brokers were up

Elasticsearch was green

Memory wasnâ€™t exploding

But the reality was painful:

ğŸ“‰ dashboards were stale
â³ lag was rising
ğŸ“¨ queues were building
ğŸ’¬ business was asking â€œwhy is data delayed?â€

Thatâ€™s when you realize:

Monitoring isnâ€™t about charts.
Itâ€™s about fast answers during failure.

Hereâ€™s how I monitor Kafka + NiFi + Elasticsearch like a production engineer.

A Day-to-Day Pipeline Youâ€™ve Definitely Built

External APIs / Apps â†’ NiFi â†’ Kafka â†’ Consumers â†’ Elasticsearch â†’ Kibana

On a normal day:
âœ… NiFi ingests
âœ… Kafka buffers
âœ… consumers index
âœ… dashboards stay fresh

Then peak traffic hits (or downstream slows).

Suddenly:

dashboards are 20â€“30 minutes behind

â€œconsumer lag increasingâ€ alerts fire

NiFi queues grow

ES indexing latency spikes

And the team starts guessing:

â€œIs Kafka slow?â€
â€œIs NiFi stuck?â€
â€œIs Elasticsearch overloaded?â€
â€œDid we lose data?â€

If you canâ€™t answer those in 5 minutes, your monitoring isnâ€™t production-grade.

The Production Monitoring Mindset

Most teams monitor:

âŒ CPU
âŒ Memory
âŒ Disk
âŒ â€œGreen cluster healthâ€

Thatâ€™s infrastructure health.

Production engineers monitor:

âœ… Flow health
âœ… Backpressure
âœ… Freshness
âœ… Throughput vs capacity
âœ… Where time is being spent

Because users donâ€™t care if CPU is 40%.

They care if dashboards are 40 minutes late.

The 3 Questions Monitoring Must Answer

Every dashboard I build is designed to answer these:

âœ… 1) Is data still entering the system?
âœ… 2) Where is it piling up?
âœ… 3) Whatâ€™s the bottleneck right now?

Everything below maps to these 3 questions.

âœ… Kafka Monitoring (Beyond â€œLag is Highâ€)
1) Consumer lag â€” but the right way

Yes, lag mattersâ€¦ but only when itâ€™s read correctly.

What I actually watch:

lag per partition

max lag partition

lag trend (slope / acceleration)

Because one hot partition can make the whole consumer group look â€œfineâ€â€¦
while one consumer is silently dying.

2) Produce rate vs consume rate

This is the math check.

If you produce faster than you consumeâ€¦
lag will always rise.

What I watch:

producer rate (msg/s)

consumer processing rate (msg/s)

time-to-catch-up estimate

This tells you instantly:
ğŸ‘‰ Can we recoverâ€¦ or are we doomed to fall behind?

3) Commit rate + rebalances

This is where stability shows up.

Signals I care about:

commits slowing down

commit latency spikes

frequent group rebalances

Frequent rebalances = throughput collapse.
And most teams only notice after lag has already blown up.

âœ… NiFi Monitoring (Where Failures Hide Quietly)

NiFi is upstream â€” and upstream failures are dangerous because they either:

stop ingestion quietly

or buffer until something breaks

1) Queue depth (count + size)

Your early warning signal.

Watch:

FlowFile count

queue size (GB)

queue growth rate

It tells you:
ğŸ‘‰ downstream is slower than ingestion, pressure is building.

2) Backpressure events

Backpressure isnâ€™t a failure.

Backpressure is protection.

Watch:

how long queues stay under backpressure

which connections trigger it

frequency under load

It tells you exactly where the system is being constrained.

3) FlowFile age (freshness)

This is the underrated NiFi metric.

Queue size can look â€œreasonableâ€â€¦
but data might be hours old.

Watch:

oldest FlowFile age per queue

This is one of the cleanest ways to measure:
ğŸ‘‰ how stale your pipeline is becoming.

âœ… Elasticsearch Monitoring (Not â€œCluster Is Greenâ€)

Elasticsearch being green doesnâ€™t mean itâ€™s healthy.

It just means shards are allocated.

1) Heap + GC pressure

Heap is the real bottleneck.

Watch:

heap % used

GC frequency / time

old-gen pressure

Most ES incidents are not CPU incidents.
Theyâ€™re heap incidents.

2) Indexing latency + write rejections

This is your first sign ES is becoming the bottleneck.

Watch:

bulk indexing latency

thread pool queue size

write rejections (429 / rejected execution)

Because this is the classic chain reaction:

ES slows â†’ consumers slow â†’ Kafka lag grows

3) Merges + Disk I/O

When merges go heavy, everything becomes spiky.

Watch:

merge backlog

merge time

disk I/O utilization

This often traces back to earlier design choices:

shard sizing

refresh intervals

ingestion style

The Most Useful Dashboard View (Single Pane That Works)

When I build a production dashboard, it always includes:

âœ… Flow health

NiFi ingest rate

Kafka produce rate

consumer processing rate

ES indexing rate

âœ… Backpressure signals

NiFi queue depth + age

Kafka lag per partition

ES indexing latency + rejections

âœ… Freshness SLA
ğŸ‘‰ â€œHow late is the data right now?â€

Because during an incident, this is the only question leadership cares about:
Are we delayed, and where is it accumulating?

The Senior Engineer Rule: Correlate, Donâ€™t Guess

Most teams see Kafka lag and immediately say:

â€œScale consumers.â€

A production engineer asks:

Did NiFi queues grow first?

Did ES indexing latency spike first?

Did write rejections start earlier?

Is one partition hot?

Because lag is often the last visible symptom.

The cause usually happened earlier.

The Goal of Monitoring Isnâ€™t Detection

The best monitoring doesnâ€™t just detect outages.

It does this:

âœ… detect degradation early
âœ… locate bottlenecks fast
âœ… contain failure
âœ… recover predictably

When itâ€™s done right:
incidents become boring.

And boring is the goal.
