# Oversharding: The Elasticsearch Mistake Everyone Makes (Once)

ğŸ‘‰ Ever had an Elasticsearch cluster that looked perfectly healthyâ€¦
but somehow got slower as you scaled?

CPU wasnâ€™t maxed.
Disk had space.
Cluster health was GREEN.

Yet:
â€¢ Indexing slowed
â€¢ Search latency crept up
â€¢ GC pauses increased

And someone said:

â€œLetâ€™s add more shards.â€

That sentence is the clue.

A Setup Youâ€™ve Definitely Built

Apps â†’ Kafka â†’ Consumers â†’ Elasticsearch â†’ Dashboards

Early days:
â€¢ Low traffic
â€¢ Few users
â€¢ Everything feels instant

So you create indices with:
â€¢ 10 shards
â€¢ Maybe 20 shards
â€¢ â€œJust in case we scaleâ€

Because youâ€™ve heard:

More shards = more parallelism

At small scale, nothing breaks.
So the decision gets locked in.

When Production Scale Arrives

A few months later:
â€¢ Traffic grows 10Ã—
â€¢ More teams query data
â€¢ Retention increases
â€¢ Dashboards refresh more often

Suddenly:
â€¢ Heap pressure rises
â€¢ Segment merges spike
â€¢ Throughput drops
â€¢ Cluster feels â€œheavyâ€

And the instinctive fix?

â€œAdd even more shards.â€

Thatâ€™s when things go sideways.

The Core Misunderstanding

What we think
â€¢ More shards = more speed
â€¢ Shards are lightweight
â€¢ Planning for scale is good
â€¢ Elasticsearch will handle it

What actually happens
â€¢ Each shard = a Lucene index
â€¢ Every shard consumes heap
â€¢ Merges multiply CPU & I/O
â€¢ Metadata grows coordination cost
â€¢ Small shards waste resources

ğŸ‘‰ Oversharding kills clusters slowly and quietly.

What a Shard Really Is

A shard is not â€œjust a bucketâ€.

Each shard has:
â€¢ Its own Lucene index
â€¢ Its own segments & merges
â€¢ Its own file handles
â€¢ Its own memory footprint

So when you create:

1 index Ã— 20 shards Ã— 30 days


You didnâ€™t create flexibility.

You created 600 Lucene indexes.

How Oversharding Shows Up in Production

â€¢ Indexing latency slowly increases
â€¢ GC pauses become frequent
â€¢ CPU looks fine, throughput drops
â€¢ Dashboards lag inconsistently
â€¢ Queries feel heavier over time

Nothing is â€œbrokenâ€.
The cluster is drowning in overhead.

Thatâ€™s why oversharding is dangerous â€”
it fails gradually, not dramatically.

The â€œGreen Clusterâ€ Trap

Cluster Health: GREEN

Green does not mean:
â€¢ Efficient
â€¢ Fast
â€¢ Well-designed

It only means:
â€¢ Shards are allocated
â€¢ Replicas are assigned

You can have a green cluster thatâ€™s one traffic spike away from collapse.

The Shard Size Reality Check

What we think

Smaller shards are safer.

What actually works

Fewer, larger shards perform better.

In production, the sweet spot is usually:
ğŸ‘‰ 20â€“50 GB per shard (often more for logs)

Large shards:
â€¢ Reduce overhead
â€¢ Reduce merges
â€¢ Use heap efficiently
â€¢ Improve cache locality

Small shards feel safe â€”
but they quietly bleed performance.

Why Oversharding Is So Hard to Fix

Hereâ€™s the painful truth:

â€¢ You can add shards later
â€¢ You cannot merge shards later

Fixing oversharding means:
â€¢ Reindexing
â€¢ Moving massive data
â€¢ Risky, expensive operations

Thatâ€™s why this is called:

The Elasticsearch mistake everyone makes once.

The Senior-Level Question

Junior engineers ask:

â€œHow many shards should we create?â€

Senior engineers ask:

â€œHow big will each shard be in 30 days?â€

That single question prevents months of pain.

ğŸ’¡ Shards are not free.
Theyâ€™re the most expensive decision you make early.

ğŸ‘‰ Which shard count decision came back to hurt you later?
ğŸ‘‡ Most of us learned this the hard way.
