# What I Look for When Reviewing a Data Pipeline Design


Iâ€™ve been in enough pipeline design reviews to know this pattern by heart:

âœ… The demo works
âœ… The architecture diagram looks clean
âœ… The happy path is perfect

â€¦and yet something in the back of your mind says:

â€œThis is going to page someone.â€

Because production doesnâ€™t care how pretty your diagram is.

Production shows up with:

messy payloads

unpredictable traffic

downstream slowness

retries that explode

â€œunknown unknownsâ€

So when I review a data pipeline design, I donâ€™t judge it by how it runsâ€¦

ğŸ‘‰ I judge it by how it fails.

A Day-to-Day Pipeline Everyone Builds

Letâ€™s say a team is building:

External API â†’ NiFi â†’ Kafka â†’ Consumers â†’ Elasticsearch â†’ Kibana

Goal sounds simple:

ingest every few minutes

power dashboards

search + visibility

near real-time results

In the meeting, the team confidently says:

â€œWeâ€™re done. Itâ€™s working in staging.â€

Thatâ€™s when my real review starts.

Because working isnâ€™t the same as surviving production traffic.

âœ… My Senior Data Engineer Review Checklist
1) Backpressure: Where Does Pressure Go When Things Slow Down?

This is always my first question.

Because something will slow down eventually:

Elasticsearch indexing gets heavy

downstream services start rejecting

partitions go hot

networking gets weird

pods restart mid-load

ğŸš© Red flags:

ingestion keeps pushing no matter what

consumers retry aggressively

downstream takes the hit

everything collapses together

âœ… What I want to see:

NiFi queues with thresholds

consumer throttling (batch + concurrency control)

lag treated as a buffer signal (not a panic number)

If your pipeline canâ€™t slow down safely, it isnâ€™t resilient. Itâ€™s just lucky.

2) Replayability: Can You Recover Without Panic?

This is the fastest maturity test:

ğŸ‘‰ If you ship a bug today, can you replay yesterdayâ€™s data?

ğŸš© Red flags:

â€œWeâ€™ll re-pull from the sourceâ€

â€œWe donâ€™t retain that longâ€

â€œWeâ€™ll patch it manuallyâ€

âœ… What I want to see:

intentional Kafka retention

safe offset reset strategy

DLQ exists with a replay path

reprocessing wonâ€™t corrupt the sink

Recovery should be a procedureâ€¦ not a project.

3) Idempotency: What Happens When Retries Happen?

Retries are not an edge case.

Theyâ€™re normal:

timeouts happen

deploys go wrong

consumers restart

ES mappings reject

â€œit worked in stagingâ€ fails in prod

ğŸš© Red flags:

retries create duplicates

dashboards drift

ES is insert-only

â€œexactly-onceâ€ is a slogan

âœ… What I want to see:

deterministic document IDs

upserts instead of blind inserts

dedupe rules documented

You donâ€™t need exactly-once claims. You need idempotent outcomes.

4) Failure Isolation: Can One Bad Record Block Everything?

One poison message shouldnâ€™t freeze the pipeline.

ğŸš© Red flags:

consumer crashes on bad payload

one bad message blocks the partition forever

â€œweâ€™ll restart itâ€ culture

âœ… What I want to see:

DLQ strategy

controlled retries â†’ then route away

alerts tied to DLQ rate/trends

replay plan documented

A pipeline is only as strong as its failure lane.

5) Throughput Strategy: How Does This Scale Under Real Load?

Scaling isnâ€™t â€œadd more pods.â€

Scaling is:

partitions support concurrency

keys distribute evenly

consumers match partition count

sinks donâ€™t get overloaded

ğŸš© Red flags:

hot partitions

guesswork partition keys

â€œweâ€™ll tune laterâ€

âœ… What I want to see:

partition strategy documented

good key distribution (avoid low-cardinality keys)

load testing done before go-live

Kafka scaling is limited by partitioning â€” not hope.

6) Elasticsearch Readiness: Are You Forcing ES to Be a Buffer?

A lot of pipelines fail because Elasticsearch is treated like a database + queue + buffer + compute engine.

ğŸš© Red flags:

ES used as a buffer

default refresh during heavy ingestion

random shard counts

no ILM

âœ… What I want to see:

bulk indexing

refresh interval tuned for ingestion

shard sizing planned

ILM defined early

Search engines should search. Kafka should buffer.

7) Observability: Can You Debug This in 5 Minutes?

When something goes wrong, the worst feeling is:

â€œWhere do we even start looking?â€

ğŸš© Red flags:

â€œweâ€™ll check logsâ€

no lag dashboards

no indexing latency visibility

no end-to-end freshness tracking

âœ… What I want to see:

consumer lag per partition

ingest rate vs processing rate

DLQ growth trends

ES indexing latency

â€œHow late is the data?â€ metric (freshness SLA)

If you canâ€™t see it, you canâ€™t own it.

8) Operational Ownership: Who Gets Paged?

This is the leadership check.

ğŸš© Red flags:

no clear owner team

tribal knowledge

no runbooks

â€œask that one guyâ€

âœ… What I want to see:

clear on-call ownership

runbook + rollback plan

replay procedures documented

SLAs agreed and realistic

If nobody owns itâ€¦ it will own you.

The Real Difference Between â€œBuiltâ€ and â€œProduction-Readyâ€

A pipeline isnâ€™t production-ready because:

it runs in staging

it moves data

dashboards look good today

A pipeline is production-ready when:

âœ… it absorbs pressure
âœ… it isolates failure
âœ… it supports replay
âœ… it stays debuggable under load
âœ… it recovers predictably

Thatâ€™s what senior engineers optimize for.

My Favorite Closing Question in Any Design Review

I always ask:

ğŸ‘‰ â€œWhat breaks firstâ€¦ and how do we recover?â€

If the answer is unclearâ€¦

the design isnâ€™t done yet.
